{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cd20cfe-ff4e-44ed-8bfb-8c6487bba8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0b3658f-0950-47af-ad32-1a18d309d3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_DATA = pd.read_csv('Employee_Profile_IT.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a9f9b6-ae71-41c9-9642-bbc0474bfd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT_FMT = '%m/%d/%Y' \n",
    "IT_DATA.columns = IT_DATA.columns.str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "IT_DATA.rename(columns={'Bonus ': 'Cur_Bonus'}, inplace=True) \n",
    "IT_DATA.rename(columns={'Exit Date': 'Init_Exit'}, inplace=True)\n",
    "IT_DATA['Annual Salary'] = IT_DATA['Annual Salary'].replace({r'[$,]': ''}, regex=True).astype(float)\n",
    "IT_DATA['Cur_Bonus'] = IT_DATA['Cur_Bonus'].astype(float) / 100 \n",
    "IT_DATA.drop('Init_Exit', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3cfd64e-33f2-46f1-a6f0-5e53da498ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daree\\AppData\\Local\\Temp\\ipykernel_13344\\1653744943.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  if IT_DATA[feat].isnull().any(): IT_DATA[feat].fillna(IT_DATA[feat].median(), inplace=True)\n",
      "C:\\Users\\daree\\AppData\\Local\\Temp\\ipykernel_13344\\1653744943.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  if IT_DATA[col].dtype == 'object' and IT_DATA[col].isnull().any(): IT_DATA[col].fillna(IT_DATA[col].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "IT_DATA = IT_DATA[~IT_DATA['EEID'].str.contains('E100', na=False)].reset_index(drop=True)\n",
    "num_feats = ['Age', 'Annual Salary', 'Cur_Bonus', 'EmploymentRating', 'DaysOfAbsence', 'CertificationsEarned']\n",
    "for feat in num_feats:\n",
    "    if IT_DATA[feat].isnull().any(): IT_DATA[feat].fillna(IT_DATA[feat].median(), inplace=True)\n",
    "for col in IT_DATA.columns:\n",
    "    if IT_DATA[col].dtype == 'object' and IT_DATA[col].isnull().any(): IT_DATA[col].fillna(IT_DATA[col].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb3f24dd-e177-49df-a205-3045f585a3c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMP_BONUS_DATA = pd.read_csv('Performance_Bonus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15cd089f-2976-48f7-a167-31658a4b4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMP_BONUS_DATA.rename(columns={'Bouns': 'Bonus_Amt'}, inplace=True) #bonus amount\n",
    "EMP_BONUS_DATA['Bonus_Amt'] = EMP_BONUS_DATA['Bonus_Amt'].str.replace('%','').astype(float) / 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1869d384-23ab-49a5-a166-c805bbc12bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "level_map = {\"Bachelor's\": 0, \"Master's\": 1, \"Doctorate\": 2}\n",
    "EMP_BONUS_DATA['EL_EN'] = EMP_BONUS_DATA['EducationLevel'].map(level_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ba8a8b6-e891-4eaa-b1f8-cd62ab21b3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1, Q3 = np.percentile(EMP_BONUS_DATA['Bonus_Amt'], [25, 75])\n",
    "IQR = Q3 - Q1\n",
    "Upper_Bound = Q3 + 1.5 * IQR\n",
    "EMP_BONUS_DATA = EMP_BONUS_DATA[(EMP_BONUS_DATA['Bonus_Amt'] <= Upper_Bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0439ec19-7674-40fe-af9a-2e65b7c3d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_COLS = ['EmploymentRating', 'DaysOfAbsence', 'CertificationsEarned', 'EL_EN']\n",
    "Y_TARG = 'Bonus_Amt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "df5214c7-c583-4429-99fa-6f3b1ac4a288",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_bonus = EMP_BONUS_DATA[X_COLS]\n",
    "Y_train_bonus = EMP_BONUS_DATA[Y_TARG]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5ebc7d34-cca9-4a9b-9b2d-44d61d5c374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_T, X_V, Y_T, Y_V = train_test_split(X_train_bonus, Y_train_bonus, test_size=0.25, random_state=35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "678e1aac-4e0c-4aff-ba1b-1aba1216d5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bonus = LinearRegression()\n",
    "model_bonus.fit(X_T, Y_T)\n",
    "Y_pred_V = model_bonus.predict(X_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0aaea813-34a3-4830-afc1-8a51d9749b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae_bonus = mean_absolute_error(y_true=Y_V, y_pred=Y_pred_V)\n",
    "mse_bonus = mean_squared_error(y_true=Y_V, y_pred=Y_pred_V)\n",
    "rmse_bonus = np.sqrt(mse_bonus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ae28adf-2085-47ce-b295-60785f924ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 10.30%\n",
      "RMSE: 11.74%\n",
      "Training Mean Bonus: 20.19%\n"
     ]
    }
   ],
   "source": [
    "print(f\"MAE: {mae_bonus * 100:.2f}%\")\n",
    "print(f\"RMSE: {rmse_bonus * 100:.2f}%\")\n",
    "print(f\"Training Mean Bonus: {(EMP_BONUS_DATA['Bonus_Amt']).mean() * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6bed0865-9b40-40d5-ab74-e534f3df2b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_DATA['EL_EN'] = IT_DATA['EducationLevel'].replace({'PhD': 'Doctorate'}).map(level_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b9fc03ec-daac-46e4-a6e7-f6ee68afd763",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daree\\AppData\\Local\\Temp\\ipykernel_13344\\26185577.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  IT_DATA['EL_EN'].fillna(impute_val, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "if IT_DATA['EL_EN'].isnull().any():\n",
    "    # Fill NaN with the mode of the encoded values\n",
    "    impute_val = IT_DATA['EL_EN'].mode()[0]\n",
    "    IT_DATA['EL_EN'].fillna(impute_val, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac45b350-a281-4f35-88ca-3015542e1a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_DATA['Predicted Bonus'] = model_bonus.predict(IT_DATA[X_COLS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "36026379-6104-4b35-88cf-b7e116490b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "IT_DATA.drop(['EL_EN'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76920dba-4683-42de-9163-725c169d37d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Predicted Bonus for IT Employees ---\n",
      "          Full Name  Cur_Bonus  Predicted Bonus\n",
      "0  Lillian Gonzales     0.0000         0.199100\n",
      "1  Scarlett Jenkins     0.0032         0.204686\n",
      "2  Brooklyn Salazar     0.0000         0.206017\n",
      "3       Riley Rojas     0.0000         0.207836\n",
      "4    Isabella Scott     0.0000         0.197077\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Predicted Bonus for IT Employees ---\")\n",
    "print(IT_DATA[['Full Name', 'Cur_Bonus', 'Predicted Bonus']].head().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04d42f83-1433-46e7-9dc2-68133a984061",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_under_rewarded = 0\n",
    "count_zero_bonus = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "566194e3-60af-48d4-a6d6-0a6f7195c6bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees who received less actual bonus than predicted: 215 (100.00%)\n",
      "Employees with zero actual bonus: 147 (68.37%)\n"
     ]
    }
   ],
   "source": [
    "for actual_bonus, predicted_bonus in zip(IT_DATA['Cur_Bonus'], IT_DATA['Predicted Bonus']):\n",
    "    if actual_bonus == 0.0:\n",
    "        count_zero_bonus += 1\n",
    "    \n",
    "    if predicted_bonus > actual_bonus:\n",
    "        count_under_rewarded += 1\n",
    "\n",
    "print(f\"Employees who received less actual bonus than predicted: {count_under_rewarded} ({count_under_rewarded / len(IT_DATA) * 100:.2f}%)\")\n",
    "print(f\"Employees with zero actual bonus: {count_zero_bonus} ({count_zero_bonus / len(IT_DATA) * 100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dd7e8f35-7f95-405a-94c3-6a7d102b6cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_filename = 'DataPredictedBonus.csv'\n",
    "IT_DATA.to_csv(output_filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "afb9ffc5-34e2-4bb6-a39d-ab4dcb8edf59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Hyperparameter Tuning Results (Ridge Regression) ---\n",
      "Optimal Alpha (Regularization): 1.0000\n",
      "Optimal Intercept Use: True\n",
      "Best cross-validation score (Negative MSE): -0.0143\n",
      "Coefficients of Best Ridge Model:\n",
      "  EmploymentRating: 0.0012\n",
      "  DaysOfAbsence: 0.0014\n",
      "  CertificationsEarned: -0.0002\n",
      "  EL_EN: 0.0027\n",
      "--- Ridge Regression Model Performance on Validation Data ---\n",
      "Mean Absolute Error (MAE): 0.1026 (Ratio) / 10.26%\n",
      "Root Mean Squared Error (RMSE): 0.1165 (Ratio) / 11.65%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 1. Define the model using a new variable\n",
    "model_ridge_new = Ridge(random_state=35)\n",
    "\n",
    "# 2. Define the hyperparameter grid to search (alpha is the regularization strength)\n",
    "param_grid_ridge = {\n",
    "    'alpha': np.logspace(-4, 0, 20), # Search for alpha between 0.0001 and 1.0\n",
    "    'fit_intercept': [True, False]\n",
    "}\n",
    "\n",
    "# 3. Setup GridSearchCV using a new variable\n",
    "# Uses the full training data (X_train_bonus, Y_train_bonus) defined in cell 10\n",
    "grid_search_ridge = GridSearchCV(\n",
    "    estimator=model_ridge_new, \n",
    "    param_grid=param_grid_ridge, \n",
    "    scoring='neg_mean_squared_error', \n",
    "    cv=5, \n",
    "    verbose=0,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# 4. Fit the grid search model to the training data\n",
    "grid_search_ridge.fit(X_train_bonus, Y_train_bonus)\n",
    "\n",
    "# 5. Extract the best model and parameters using new variables\n",
    "best_alpha_ridge = grid_search_ridge.best_params_['alpha']\n",
    "best_intercept_ridge = grid_search_ridge.best_params_['fit_intercept']\n",
    "best_model_ridge = grid_search_ridge.best_estimator_\n",
    "\n",
    "print(f\"--- Hyperparameter Tuning Results (Ridge Regression) ---\")\n",
    "print(f\"Optimal Alpha (Regularization): {best_alpha_ridge:.4f}\")\n",
    "print(f\"Optimal Intercept Use: {best_intercept_ridge}\")\n",
    "print(f\"Best cross-validation score (Negative MSE): {grid_search_ridge.best_score_:.4f}\")\n",
    "print(f\"Coefficients of Best Ridge Model:\")\n",
    "print(f\"  {X_COLS[0]}: {best_model_ridge.coef_[0]:.4f}\")\n",
    "print(f\"  {X_COLS[1]}: {best_model_ridge.coef_[1]:.4f}\")\n",
    "print(f\"  {X_COLS[2]}: {best_model_ridge.coef_[2]:.4f}\")\n",
    "print(f\"  {X_COLS[3]}: {best_model_ridge.coef_[3]:.4f}\")\n",
    "Y_pred_V_ridge = best_model_ridge.predict(X_V)\n",
    "mae_ridge = mean_absolute_error(y_true=Y_V, y_pred=Y_pred_V_ridge)\n",
    "mse_ridge = mean_squared_error(y_true=Y_V, y_pred=Y_pred_V_ridge)\n",
    "rmse_ridge = np.sqrt(mse_ridge)\n",
    "print(f\"--- Ridge Regression Model Performance on Validation Data ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_ridge:.4f} (Ratio) / {mae_ridge * 100:.2f}%\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_ridge:.4f} (Ratio) / {rmse_ridge * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "7f93b17a-74c8-4131-92bc-11237b4c6f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tuned XGBoost Regression Model ---\n",
      "Optimal XGBoost Parameters found: {'reg__subsample': 0.9, 'reg__n_estimators': 100, 'reg__max_depth': 3, 'reg__learning_rate': 0.01, 'reg__colsample_bytree': 0.9}\n",
      "Best cross-validation score (Negative MSE): -0.0143\n",
      "\n",
      "--- Model Performance on Validation Data ---\n",
      "Mean Absolute Error (MAE): 0.1006 (Ratio) / 10.06%\n",
      "Root Mean Squared Error (RMSE): 0.1147 (Ratio) / 11.47%\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Define the features to use (using the original categorical column 'EducationLevel')\n",
    "X_COLS_XGB = ['EmploymentRating', 'DaysOfAbsence', 'CertificationsEarned', 'EducationLevel']\n",
    "Y_TARG = 'Bonus_Amt' # Target is still the bonus ratio\n",
    "# Use the full, cleaned data (EMP_BONUS_DATA) for the tuning process\n",
    "X_train_xgb = EMP_BONUS_DATA[X_COLS_XGB]\n",
    "Y_train_xgb = EMP_BONUS_DATA[Y_TARG]\n",
    "# Split the data into train and validation sets for final evaluation\n",
    "# Use new variables X_T_xgb and X_V_xgb to avoid overwriting original split\n",
    "X_T_xgb, X_V_xgb, Y_T_xgb, Y_V_xgb = train_test_split(\n",
    "    X_train_xgb, Y_train_xgb, test_size=0.25, random_state=35 # Using the same random state as original split)\n",
    "# 1. Define the preprocessing pipeline for XGBoost\n",
    "NUMERIC_FEATURES = ['EmploymentRating', 'DaysOfAbsence', 'CertificationsEarned']\n",
    "CATEGORICAL_FEATURES = ['EducationLevel']\n",
    "preproc_xgb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Numerical features passed through without scaling\n",
    "        ('num', 'passthrough', NUMERIC_FEATURES),\n",
    "        # Categorical features encoded\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), CATEGORICAL_FEATURES)\n",
    "    ],\n",
    "    remainder='drop' # Drop any other columns not specified)\n",
    "# 2. Define the full model pipeline (Preprocessing + Regressor)\n",
    "model_xgb_pipeline = Pipeline(steps=[\n",
    "    ('preproc', preproc_xgb),\n",
    "    ('reg', XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        random_state=35,\n",
    "        n_jobs=-1,\n",
    "        # Set silent=True to avoid printing messages during tuning\n",
    "        # 'verbose': 0 (or remove completely) is typically used for silencing XGBoost\n",
    "    ))])\n",
    "# 3. Define the parameter distribution for Randomized Search\n",
    "param_dist_xgb = {\n",
    "    'reg__n_estimators': [100, 300, 500, 700],\n",
    "    'reg__max_depth': [3, 5, 7],\n",
    "    'reg__learning_rate': [0.01, 0.05, 0.1],\n",
    "    'reg__subsample': [0.7, 0.9, 1.0],\n",
    "    'reg__colsample_bytree': [0.7, 0.9, 1.0]}\n",
    "# 4. Setup RandomizedSearchCV using the entire training data for internal CV\n",
    "# Use 30 iterations for a reasonable search time\n",
    "random_search_xgb = RandomizedSearchCV(\n",
    "    model_xgb_pipeline,\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=30, # Test 30 random combinations\n",
    "    scoring='neg_mean_squared_error',\n",
    "    cv=5, # 5-fold cross-validation\n",
    "    verbose=0, # Set to 0 to minimize output during search\n",
    "    random_state=35,\n",
    "    n_jobs=-1)\n",
    "# Fit the grid search model (tuning and training the best model)\n",
    "# Note: X_train_xgb and Y_train_xgb are the full training sets used for tuning.\n",
    "random_search_xgb.fit(X_train_xgb, Y_train_xgb)\n",
    "# 5. Extract the best model and parameters\n",
    "best_xgb_model_tuned = random_search_xgb.best_estimator_\n",
    "best_xgb_params = random_search_xgb.best_params_\n",
    "# 6. Predict and evaluate performance on the held-out validation set (X_V_xgb)\n",
    "Y_pred_V_xgb_tuned = best_xgb_model_tuned.predict(X_V_xgb)\n",
    "# Ensure predictions are non-negative since bonus ratio cannot be negative\n",
    "Y_pred_V_xgb_tuned[Y_pred_V_xgb_tuned < 0] = 0\n",
    "# Calculate MAE and RMSE\n",
    "mae_xgb_tuned = mean_absolute_error(Y_V_xgb, Y_pred_V_xgb_tuned)\n",
    "rmse_xgb_tuned = np.sqrt(mean_squared_error(Y_V_xgb, Y_pred_V_xgb_tuned))\n",
    "# 7. Print the results\n",
    "print(f\"--- Tuned XGBoost Regression Model ---\")\n",
    "print(f\"Optimal XGBoost Parameters found: {best_xgb_params}\")\n",
    "print(f\"Best cross-validation score (Negative MSE): {random_search_xgb.best_score_:.4f}\")\n",
    "print(f\"\\n--- Model Performance on Validation Data ---\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae_xgb_tuned:.4f} (Ratio) / {mae_xgb_tuned * 100:.2f}%\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse_xgb_tuned:.4f} (Ratio) / {rmse_xgb_tuned * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "28a4c715-e6bc-4500-8949-0e171cf62e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- IT Data with Tuned XGBoost Predictions ---\n",
      "          Full Name  Cur_Bonus  Predicted Bonus  Predicted Bonus (XGBoost Tuned)\n",
      "0  Lillian Gonzales     0.0000         0.199100                         0.201252\n",
      "1  Scarlett Jenkins     0.0032         0.204686                         0.209264\n",
      "2  Brooklyn Salazar     0.0000         0.206017                         0.202194\n",
      "3       Riley Rojas     0.0000         0.207836                         0.209642\n",
      "4    Isabella Scott     0.0000         0.197077                         0.205071\n",
      "\n",
      "Final output saved to: DataPredictedBonus_enhanced.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\daree\\AppData\\Local\\Temp\\ipykernel_13344\\1786896521.py:10: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  IT_DATA['Predicted Bonus (XGBoost Tuned)'].clip(lower=0, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. Define the features required by the tuned XGBoost model's preprocessing pipeline\n",
    "# These are the original columns used for training the best_xgb_model_tuned:\n",
    "X_COLS_XGB_FINAL = ['EmploymentRating', 'DaysOfAbsence', 'CertificationsEarned', 'EducationLevel']\n",
    "\n",
    "# 2. Use the best tuned model (which includes the ColumnTransformer) to predict the bonus ratio\n",
    "# The best_xgb_model_tuned handles the transformation of 'EducationLevel' internally.\n",
    "IT_DATA['Predicted Bonus (XGBoost Tuned)'] = best_xgb_model_tuned.predict(IT_DATA[X_COLS_XGB_FINAL])\n",
    "\n",
    "# 3. Ensure predictions are non-negative since bonus ratio cannot be negative\n",
    "IT_DATA['Predicted Bonus (XGBoost Tuned)'].clip(lower=0, inplace=True)\n",
    "\n",
    "# 4. Display the head of the updated DataFrame for verification\n",
    "print(\"\\n--- IT Data with Tuned XGBoost Predictions ---\")\n",
    "print(IT_DATA[['Full Name', 'Cur_Bonus', 'Predicted Bonus', 'Predicted Bonus (XGBoost Tuned)']].head().to_string())\n",
    "\n",
    "# 5. Generate the final CSV file\n",
    "output_filename_xgb = 'DataPredictedBonus_enhanced.csv'\n",
    "IT_DATA.to_csv(output_filename_xgb, index=False)\n",
    "\n",
    "print(f\"\\nFinal output saved to: {output_filename_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8358c4e7-0b58-4ae4-8427-b3c93f00864b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
